{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dermatologist-AI",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMSuQL+CRJJLcHORGX6nyHm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anjosmelanie/cd0281-Introduction-to-Neural-Networks-with-PyTorch/blob/master/Dermatologist_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rPZAT6lyXM-R"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Dataset\n",
        "from torchvision import datasets, transforms, models \n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "\n",
        "# Training Model\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Extra Information\n",
        "from PIL import Image\n",
        "import tqdm \n",
        "from torchsummary import summary\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Download datasets, once per session only\n",
        "import urllib\n",
        "\n",
        "train_url = 'https://s3-us-west-1.amazonaws.com/udacity-dlnfd/datasets/skin-cancer/train.zip'\n",
        "valid_url = 'https://s3-us-west-1.amazonaws.com/udacity-dlnfd/datasets/skin-cancer/valid.zip'\n",
        "test_url = 'https://s3-us-west-1.amazonaws.com/udacity-dlnfd/datasets/skin-cancer/test.zip'\n",
        "\n",
        "def download(url, zip):\n",
        "  urllib.request.urlretrieve(url, zip)\n",
        "  print(zip + ' downloaded successfully!')\n",
        "\n",
        "download(train_url, 'train.zip')\n",
        "! unzip 'train.zip'\n",
        "\n",
        "download(valid_url, 'valid.zip')\n",
        "! unzip 'valid.zip'\n",
        "\n",
        "download(test_url, 'test.zip')\n",
        "! unzip 'test.zip'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85KBGmLVk7y4",
        "outputId": "2fe896ec-245b-4dad-ba79-dc9bc9f8364e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train.zip downloaded successfully!\n",
            "Archive:  train.zip\n",
            "replace train/melanoma/ISIC_0010034.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO: Eventually mix up traning + valid data and split them\n",
        "\n",
        "train_path = \"train\"\n",
        "valid_path = \"valid\"\n",
        "test_path = \"test\"\n",
        "\n",
        "image_size = 224\n",
        "batch_size = 20\n",
        "valid_percent = 0.2\n",
        "\n",
        "mean=[0.485, 0.456, 0.406]\n",
        "std=[0.229, 0.224, 0.225]\n",
        "\n",
        "# ------------------------------------------------ #\n",
        "\n",
        "# Data transform\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomRotation(45),\n",
        "    transforms.RandomResizedCrop(image_size),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "# ------------------------------------------------ #\n",
        "\n",
        "# Dataset\n",
        "# Load and transform data using ImageFolder\n",
        "train_data = datasets.ImageFolder(train_path, transform = transform_test)\n",
        "print('Number of training images: ', len(train_data))\n",
        "\n",
        "valid_data = datasets.ImageFolder(valid_path, transform = transform_test)\n",
        "print('Number of validation images: ', len(train_data))\n",
        "\n",
        "test_data = datasets.ImageFolder(test_path, transform = transform_test)\n",
        "print('Number of test images: ', len(test_data))\n",
        "\n",
        "# ------------------------------------------------ #\n",
        "\n",
        "# Prepare data loaders\n",
        "train_data_loader = DataLoader(train_data, batch_size=batch_size)\n",
        "valid_data_loader = DataLoader(valid_data, batch_size=batch_size)\n",
        "test_data_loader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "# ------------------------------------------------ #\n",
        "\n",
        "loaders = {'train': train_data_loader, 'valid': valid_data_loader, 'test': test_data_loader}\n"
      ],
      "metadata": {
        "id": "TZ75ehF-XgE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes_raw = train_data.classes\n",
        "#classes = [' '.join(_c[3:].split('_')) for _c in classes_raw] # Label formatting (remove number and replace '_' for spaces)\n",
        "num_classes = len(classes_raw)\n",
        "\n",
        "# Visualize one batch\n",
        "img, label = next(iter(train_data_loader))\n",
        "fig = plt.figure(figsize=(16,12))\n",
        "\n",
        "for i in range(batch_size):\n",
        "    single_image = img[i]\n",
        "    label_axis = plt.subplot(4, 5, i + 1)\n",
        "    single_image = single_image * 0.5 + 0.5\n",
        "    plt.imshow(np.transpose(single_image, (1,2,0)))\n",
        "    label_axis.set_title(classes_raw[label[i]])\n",
        "    label_axis.axis('off')"
      ],
      "metadata": {
        "id": "rDQF7nxekFoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# useful variable that tells us whether we should use the GPU\n",
        "use_cuda = torch.cuda.is_available()\n",
        "print(use_cuda)"
      ],
      "metadata": {
        "id": "u0jA8pyNqEmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Loss Function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "## Optimizer\n",
        "def get_optimizer(model):\n",
        "      params_to_update = []\n",
        "      for name, param in model.named_parameters():\n",
        "          if param.requires_grad == True:\n",
        "              params_to_update.append(param)\n",
        "      \n",
        "      return optim.Adam(params_to_update, lr=0.00015) "
      ],
      "metadata": {
        "id": "vzhTQ-qEqR6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Architecture\n",
        "\n",
        "def get_transfered_model_resnet50():\n",
        "\n",
        "    model_transfer = models.resnet50(pretrained=True)\n",
        "    \n",
        "    for param in model_transfer.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    model_transfer.fc = nn.Linear(2048, num_classes)\n",
        "    \n",
        "    optimizer = get_optimizer(model_transfer)\n",
        "    \n",
        "    return model_transfer, optimizer\n",
        "\n",
        "## Specify model architecture\n",
        "\n",
        "model_transfer, optimizer = get_transfered_model_resnet50()\n",
        "\n",
        "print(model_transfer)"
      ],
      "metadata": {
        "id": "ptn-rhTGqgCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Train the model and save the best model parameters at filepath 'model_transfer.pt'\n",
        "\n",
        "def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path):\n",
        "    \"\"\"returns trained model\"\"\"\n",
        "    # initialize tracker for minimum validation loss\n",
        "    valid_loss_min = np.Inf \n",
        "    \n",
        "    # scheduler to decrease the learning rate\n",
        "    scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "    \n",
        "    # ------------------------------------------------ #\n",
        "    \n",
        "    for epoch in range(1, n_epochs+1):\n",
        "        # initialize variables to monitor training and validation loss\n",
        "        train_loss = 0.0\n",
        "        valid_loss = 0.0\n",
        "        \n",
        "        # set the module to training mode\n",
        "        model.train()\n",
        "        for batch_idx, (data, target) in enumerate(tqdm.tqdm(loaders['train'])):\n",
        "            # move to GPU\n",
        "            if use_cuda:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "\n",
        "            ## find the loss and update the model parameters accordingly\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Update the train_loss\n",
        "            train_loss = 1./(batch_idx+1) * (batch_idx*train_loss + loss.data.item())\n",
        "            \n",
        "        scheduler.step()\n",
        "\n",
        "        # set the model to evaluation mode\n",
        "        model.eval()\n",
        "        for batch_idx, (data, target) in enumerate(loaders['valid']):\n",
        "            # move to GPU\n",
        "            if use_cuda:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "\n",
        "            ## update average validation loss \n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            valid_loss = 1./(batch_idx+1) * (batch_idx*valid_loss + loss.data.item())\n",
        "\n",
        "\n",
        "        # print training/validation statistics \n",
        "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "            epoch, \n",
        "            train_loss,\n",
        "            valid_loss\n",
        "            ))\n",
        "\n",
        "        ## if the validation loss has decreased, save the model at the filepath stored in save_path\n",
        "        if valid_loss < valid_loss_min:\n",
        "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min, valid_loss))\n",
        "            valid_loss_min = valid_loss\n",
        "            valid_loss_min_epoch = epoch\n",
        "            torch.save(model.state_dict(), save_path)   \n",
        "            \n",
        "        # ------------------------------------------------ #\n",
        "            \n",
        "        # Early Stopping    \n",
        "            \n",
        "        if early_stopping(epoch, valid_loss, valid_loss_min, valid_loss_min_epoch, 5, 0.05):\n",
        "            break\n",
        "        else:\n",
        "            pass\n",
        "                \n",
        "        # ------------------------------------------------ #\n",
        "        \n",
        "    return model\n",
        "\n",
        "    # ------------------------------------------------ #\n",
        "    \n",
        "def early_stopping(epoch, valid_loss, valid_loss_min, valid_loss_min_epoch, patience, stop_th):    \n",
        "        if (epoch - valid_loss_min_epoch) > patience:\n",
        "            # Stop the training if the current loss is greater than the threshold, or just keep training \n",
        "            if (valid_loss - valid_loss_min) > (stop_th * valid_loss_min): \n",
        "                print(\"The validation loss jumped across the threshold, stop.\")\n",
        "                return True\n",
        "            else:\n",
        "                return False    \n",
        "\n",
        "    # ------------------------------------------------ #\n",
        "    \n",
        "def transfer_weight_init(m):\n",
        "## implement a weight initialization strategy\n",
        "    class_name = m.__class__.__name__\n",
        "    if class_name.find(\"Linear\") != -1:\n",
        "        n = m.in_features\n",
        "        y = 1.0/np.sqrt(n)\n",
        "        m.weight.data.normal_(0, y)\n",
        "        m.bias.data.fill_(0)\n",
        "        \n",
        "    # ------------------------------------------------ #\n",
        "    \n",
        "num_epochs = 25\n",
        "\n",
        "model_transfer.apply(transfer_weight_init)\n",
        "\n",
        "model_transfer = train(num_epochs, loaders, model_transfer, get_optimizer(model_transfer), criterion, use_cuda, 'model_transfer.pt') \n",
        "    \n",
        "# load the model that got the best validation accuracy\n",
        "model_transfer.load_state_dict(torch.load('model_transfer.pt'))\n"
      ],
      "metadata": {
        "id": "moFsXqCHrZ30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(loaders, model, criterion, use_cuda):\n",
        "\n",
        "    # monitor test loss and accuracy\n",
        "    test_loss = 0.\n",
        "    correct = 0.\n",
        "    total = 0.\n",
        "\n",
        "    # set the module to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(tqdm.tqdm(loaders['test'])):\n",
        "        # move to GPU\n",
        "        if use_cuda:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(data)\n",
        "        # calculate the loss\n",
        "        loss = criterion(output, target)\n",
        "        # update average test loss \n",
        "        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data.item() - test_loss))\n",
        "        # convert output probabilities to predicted class\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        # compare predictions to true label\n",
        "        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
        "        total += data.size(0)\n",
        "            \n",
        "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
        "        100. * correct / total, correct, total))\n",
        "\n",
        "# load the model that got the best validation accuracy\n",
        "model_transfer.load_state_dict(torch.load('model_transfer.pt'))\n",
        "test(loaders, model_transfer, criterion, use_cuda)"
      ],
      "metadata": {
        "id": "HDWOFnat84zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_diagnosis(img_path):\n",
        "     \n",
        "    # Open and transform image\n",
        "    image_size = 224\n",
        "    normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    transform_image = transforms.Compose([transforms.Resize((image_size,image_size)), transforms.ToTensor(), normalize])\n",
        "    \n",
        "    raw_image = Image.open(img_path)\n",
        "    transformed_image = transform_image(raw_image)\n",
        "    transformed_image = transformed_image.unsqueeze(0)\n",
        "    if use_cuda:\n",
        "        transformed_image = transformed_image.cuda()  \n",
        "    \n",
        "    # Load model and make a prediction\n",
        "    model_transfer.load_state_dict(torch.load('model_transfer.pt'))\n",
        "    model_transfer.eval()\n",
        "    output = model_transfer(transformed_image)\n",
        "\n",
        "    probs = torch.nn.functional.softmax(output, dim=1)\n",
        "    classes = list(train_data.class_to_idx.keys())\n",
        "\n",
        "    #for i in range(0,3):\n",
        "    #  print(\"Probability of\", classes[i], \": \", probs[0][i].item())\n",
        "\n",
        "    prob_melanoma =  probs[0][0].item()\n",
        "    prob_novus =  probs[0][1].item()\n",
        "    prob_keratosis =  probs[0][2].item()\n",
        "\n",
        "    #print(img_path, prob_melanoma, prob_keratosis)\n",
        "\n",
        "    return img_path, prob_melanoma, prob_keratosis\n",
        "\n",
        "# Testing Predictions\n",
        "image_path1 = 'test/seborrheic_keratosis/ISIC_0012974.jpg'\n",
        "image_path2 = 'test/nevus/ISIC_0015631.jpg'\n",
        "print(predict_diagnosis(image_path1))\n",
        "print(predict_diagnosis(image_path2))"
      ],
      "metadata": {
        "id": "UB5Vjy-0a7EY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "header = ['Id', 'task_1', 'task_2']\n",
        "\n",
        "data = []\n",
        "for i in range(0,600):\n",
        "  data.append(predict_diagnosis(test_data.imgs[i][0]))\n",
        "\n",
        "with open('predictions.csv', 'w', encoding='UTF8') as f:\n",
        "    writer = csv.writer(f)\n",
        "\n",
        "    # write the header\n",
        "    writer.writerow(header)\n",
        "\n",
        "     # write multiple rows\n",
        "    writer.writerows(data)\n",
        "\n",
        "# close the file\n",
        "f.close()"
      ],
      "metadata": {
        "id": "T8JJT5oGC1rm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}